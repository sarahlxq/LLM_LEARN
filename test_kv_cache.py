import argparse

def estimate_kv_cache(
    batch_size: int,
    max_seq_len: int,
    num_layers: int,
    num_heads: int,
    head_dim: int,
    dtype: str = "float16",
    gqa: bool = False,
    num_key_value_heads: int = None,
    verbose: bool = True
):
    """
    ä¼°ç®— KV Cache çš„æ˜¾å­˜å ç”¨ã€‚
    
    å‚æ•°è¯´æ˜ï¼š
    - batch_size: å¹¶å‘è¯·æ±‚æ•°
    - max_seq_len: æœ€å¤§åºåˆ—é•¿åº¦ï¼ˆprompt + ç”Ÿæˆ tokenï¼‰
    - num_layers: æ¨¡å‹å±‚æ•°ï¼ˆå¦‚ LLaMA-7B æ˜¯ 32 å±‚ï¼‰
    - num_heads: æ¯å±‚çš„æ³¨æ„åŠ›å¤´æ•°ï¼ˆå¯¹äº GQAï¼Œè¿™æ˜¯ query heads æ•°é‡ï¼‰
    - head_dim: æ¯ä¸ª attention head çš„ç»´åº¦ï¼ˆå¦‚ 128ï¼‰
    - dtype: æ•°æ®ç±»å‹ï¼Œæ”¯æŒ float16 / bfloat16 / float32
    - gqa: æ˜¯å¦ä½¿ç”¨ Grouped Query Attention
    - num_key_value_heads: GQA ä¸­ Key/Value çš„å¤´æ•°é‡ï¼ˆå¦‚ 8ï¼‰
    - verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
    """

    # æ•°æ®ç±»å‹å­—èŠ‚æ•°
    dtype_size_map = {
        "float16": 2,
        "bfloat16": 2,
        "float32": 4,
    }
    if dtype not in dtype_size_map:
        raise ValueError(f"Unsupported dtype: {dtype}")
    dtype_size = dtype_size_map[dtype]

    # å¦‚æœä½¿ç”¨ GQAï¼Œåˆ™ key/value çš„ head æ•°å°äº query çš„ head æ•°
    if gqa:
        if num_key_value_heads is None:
            raise ValueError("GQA éœ€è¦æä¾› num_key_value_heads")
        kv_heads = num_key_value_heads
    else:
        kv_heads = num_heads

    # å•ä¸ª token çš„ KV ç¼“å­˜å¤§å°ï¼ˆæ¯ä¸ª layerï¼‰
    per_token_kv_size_per_layer = 2 * kv_heads * head_dim  # K + V

    # æ¯ä¸ªè¯·æ±‚æ¯å±‚çš„æœ€å¤§ç¼“å­˜å¤§å°
    per_request_per_layer_kv = max_seq_len * per_token_kv_size_per_layer

    # æ€»ä½“ KV Cache å¤§å°ï¼ˆæ‰€æœ‰ batch + æ‰€æœ‰å±‚ï¼‰
    total_kv_cache_bytes = (
        batch_size *
        num_layers *
        per_request_per_layer_kv *
        dtype_size
    )

    # è½¬æ¢ä¸º GB
    total_kv_cache_gb = total_kv_cache_bytes / (1024 ** 3)

    if verbose:
        print("\nğŸ“Š KV Cache æ˜¾å­˜ä¼°ç®—ç»“æœ:")
        print(f"  - å¹¶å‘è¯·æ±‚æ•° (batch_size): {batch_size}")
        print(f"  - æœ€å¤§åºåˆ—é•¿åº¦ (max_seq_len): {max_seq_len}")
        print(f"  - æ¨¡å‹å±‚æ•° (num_layers): {num_layers}")
        print(f"  - æ³¨æ„åŠ›å¤´æ•° (num_heads): {num_heads}")
        if gqa:
            print(f"  - Key/Value å¤´æ•° (num_key_value_heads): {num_key_value_heads}")
        print(f"  - æ¯å¤´ç»´åº¦ (head_dim): {head_dim}")
        print(f"  - æ•°æ®ç±»å‹ (dtype): {dtype}")
        print(f"\nğŸ§  æ€»è®¡ KV Cache æ˜¾å­˜å ç”¨ â‰ˆ {total_kv_cache_gb:.2f} GB")

    return total_kv_cache_gb


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="ä¼°ç®— KV Cache æ˜¾å­˜å ç”¨")
    parser.add_argument("--batch_size", type=int, default=1)
    parser.add_argument("--max_seq_len", type=int, default=4096)
    parser.add_argument("--num_layers", type=int, default=32)
    parser.add_argument("--num_heads", type=int, default=32)
    parser.add_argument("--head_dim", type=int, default=128)
    parser.add_argument("--dtype", type=str, default="float16", choices=["float16", "bfloat16", "float32"])
    parser.add_argument("--gqa", action="store_true", help="æ˜¯å¦å¯ç”¨ GQA")
    parser.add_argument("--num_key_value_heads", type=int, default=None)
    args = parser.parse_args()

    estimate_kv_cache(**vars(args))
